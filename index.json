[{"authors":["admin"],"categories":null,"content":"Yuze Chi (Blaok) obtained his PhD in Computer Science Department, UCLA. Yuze has been working on various computer system optimization projects in many big data applications, including graph processing, image processing, and genomics. Beyond building optimized solutions for each application one by one, Yuze also worked on unified programming infrastructures for heterogeneous systems. After graduation, Yuze joined Google and started the quest to handle and optimize computing and networking infrastructures at Google scale. ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1683607348,"objectID":"7e236544b9216eca9c0e351b992deeec","permalink":"https://about.blaok.me/authors/blaok/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/blaok/","section":"authors","summary":"Yuze Chi (Blaok) obtained his PhD in Computer Science Department, UCLA. Yuze has been working on various computer system optimization projects in many big data applications, including graph processing, image processing, and genomics. Beyond building optimized solutions for each application one by one, Yuze also worked on unified programming infrastructures for heterogeneous systems. After graduation, Yuze joined Google and started the quest to handle and optimize computing and networking infrastructures at Google scale.","tags":null,"title":"Yuze Chi","type":"authors"},{"authors":["Moazin Khatti","Xingyu Tian","**Yuze Chi**","Licheng Guo","Jason Cong","Zhenman Fang"],"categories":null,"content":"","date":1683504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684296856,"objectID":"fde1eacb44ee1a37eb56db0acbfce563","permalink":"https://about.blaok.me/publication/fccm23-pasta/","publishdate":"2023-05-08T00:00:00Z","relpermalink":"/publication/fccm23-pasta/","section":"publication","summary":"In recent years, there has been increasing adoption of FPGAs in datacenters as hardware accelerators, where a large population of end users are software developers. While high-level synthesis (HLS) facilitates software programming, it is still challenging to scale large accelerator designs on modern datacenter FPGAs that often consist of multiple dies and memory banks. More specifically, routing congestion and extra delays on these multi-die FPGAs often cause timing closure issues and severe frequency degradation at the physical design level, which are difficult to digest and optimize for high-level programmers using HLS. One promising approach to mitigate such issues is to develop a high-level task-parallel programming model with HLS and physical design co-optimization. Unfortunately, existing studies only support a programming model where tasks communicate with each other via FIFOs, while many applications are not streaming friendly and many existing accelerator designs heavily rely on buffer based communication between tasks.  In this paper, we take a step further to support a task-parallel programming model where tasks can communicate via both FIFOs and buffers. To achieve this goal, we design and implement the PASTA framework, which takes a large task-parallel HLS design as input and automatically generates a high-frequency FPGA accelerator via HLS and physical design co-optimization. First, we design a decoupled latency-insensitive buffer channel that supports memory partitioning and ping-pong buffering, which is compatible with the vendor Vitis HLS compiler. In the frontend, we develop an easy-to-use programming interface to allow end users to use our buffer channel in their applications. In the backend, we provide automatic coarse-grained floorplanning and pipelining for designs that use our proposed buffer channel. We test PASTA on a set of task-parallel HLS designs that use buffers for task communication and show an average of 36% (up to 54%) frequency improvement for large design configurations.","tags":null,"title":"PASTA: Programming and Automation Support for Scalable Task-Parallel HLS Programs on Modern Multi-Die FPGAs","type":"publication"},{"authors":["Licheng Guo","Pongstorn Maidee","Yun Zhou","Chris Lavin","Eddie Hung","Wuxi Li","Jason Lau","Weikang Qiao","**Yuze Chi**","Linghao Song","Yuanlong Xiao","Alireza Kaviani","Zhiru Zhang","Jason Cong"],"categories":null,"content":"","date":1682467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683610317,"objectID":"0c5bd57547f9dcc0331e0705db80c640","permalink":"https://about.blaok.me/publication/trets23-rapidstream/","publishdate":"2023-04-26T00:00:00Z","relpermalink":"/publication/trets23-rapidstream/","section":"publication","summary":"FPGAs require a much longer compilation cycle than conventional computing platforms like CPUs. In this paper, we shorten the overall compilation time by co-optimizing the HLS compilation (C-to-RTL) and the back-end physical implementation (RTL-to-bitstream). We propose a split compilation approach based on the pipelining flexibility at the HLS level, which allows us to partition designs for parallel placement and routing. We outline a number of technical challenges and address them by breaking the conventional boundaries between different stages of the traditional FPGA tool flow and reorganizing them to achieve a fast end-to-end compilation.  Our research produces RapidStream, a parallelized and physical-integrated compilation framework that takes in a latency-insensitive program in C/C++ and generates a fully placed and routed implementation. We present two approaches. The first approach (RapidStream 1.0) resolves inter-partition routing conflicts at the end when separate partitions are stitched together. When tested on the Xilinx U250 FPGA with a set of realistic HLS designs, RapidStream achieves a 5-7 × reduction in compile time and up to 1.3 × increase in frequency when compared to a commercial off-the-shelf toolchain. In addition, we provide preliminary results using a customized open-source router to reduce the compile time up to an order of magnitude in cases with lower performance requirements. The second approach (RapidStream 2.0) prevents routing conflicts using virtual pins. Testing on Xilinx U280 FPGA, we observed 5-7 × compile time reduction and 1.3 × frequency increase.","tags":null,"title":"RapidStream 2.0: Automated Parallel Implementation of Latency Insensitive FPGA Designs Through Partial Reconfiguration","type":"publication"},{"authors":["Xingyu Tian","Zhifan Ye","Alec Lu","Licheng Guo","**Yuze Chi**","Zhenman Fang"],"categories":null,"content":"","date":1681689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683695564,"objectID":"68f54cd8f27efee54b71a2198d74055d","permalink":"https://about.blaok.me/publication/trets23-sasa/","publishdate":"2023-04-17T00:00:00Z","relpermalink":"/publication/trets23-sasa/","section":"publication","summary":"Stencil computation is one of the fundamental computing patterns in many application domains such as scientific computing and image processing. While there are promising studies that accelerate stencils on FPGAs, there lacks an automated acceleration framework to systematically explore both spatial and temporal parallelisms for iterative stencils that could be either computation-bound or memory-bound. In this article, we present SASA, a scalable and automatic stencil acceleration framework on modern HBM-based FPGAs. SASA takes the high-level stencil DSL and FPGA platform as inputs, automatically exploits the best spatial and temporal parallelism configuration based on our accurate analytical model, and generates the optimized FPGA design with the best parallelism configuration in TAPA high-level synthesis C++ as well as its corresponding host code. Compared to state-of-the-art automatic stencil acceleration framework SODA that only exploits temporal parallelism, SASA achieves an average speedup of 3.41× and up to 15.73× speedup on the HBM-based Xilinx Alveo U280 FPGA board for a wide range of stencil kernels.","tags":null,"title":"SASA: A Scalable and Automatic Stencil Acceleration Framework for Optimized Hybrid Spatial and Temporal Parallelism on HBM-based FPGAs","type":"publication"},{"authors":["Linghao Song","Licheng Guo","Suhail Basalama","**Yuze Chi**","Robert F. Lucas","Jason Cong"],"categories":null,"content":"","date":1676160000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683610567,"objectID":"cb66015c08e18c328ef237d86c118ba4","permalink":"https://about.blaok.me/publication/fpga23-callipepla/","publishdate":"2023-02-12T00:00:00Z","relpermalink":"/publication/fpga23-callipepla/","section":"publication","summary":"The continued growth in the processing power of FPGAs coupled with high bandwidth memories (HBM), makes systems like the Xilinx U280 credible platforms for linear solvers which often dominate the run time of scientific and engineering applications. In this paper, we present Callipepla, an accelerator for a preconditioned conjugate gradient linear solver (CG). FPGA acceleration of CG faces three challenges: (1) how to support an arbitrary problem and terminate acceleration processing on the fly, (2) how to coordinate long-vector data flow among processing modules, and (3) how to save off-chip memory bandwidth and maintain double (FP64) precision accuracy. To tackle the three challenges, we present (1) a stream-centric instruction set for efficient streaming processing and control, (2) vector streaming reuse (VSR) and decentralized vector flow scheduling to coordinate vector data flow among modules and further reduce off-chip memory access latency with a double memory channel design, and (3) a mixed precision scheme to save bandwidth yet still achieve effective double precision quality solutions. To the best of our knowledge, this is the first work to introduce the concept of VSR for data reusing between on-chip modules to reduce unnecessary off-chip accesses and enable modules working in parallel for FPGA accelerators. We prototype the accelerator on a Xilinx U280 HBM FPGA. Our evaluation shows that compared to the Xilinx HPC product, the XcgSolver, Callipepla achieves a speedup of 3.94x, 3.36x higher throughput, and 2.94x better energy efficiency. Compared to an NVIDIA A100 GPU which has 4x the memory bandwidth of Callipepla, we still achieve 77% of its throughput with 3.34x higher energy efficiency. The code is available at https://github.com/UCLA-VAST/Callipepla.","tags":null,"title":"Callipepla: Stream Centric Instruction Set and Mixed Precision for Accelerating Conjugate Gradient Solver","type":"publication"},{"authors":["**Yuze Chi**","Weikang Qiao","Atefeh Sohrabizadeh","Jie Wang","Jason Cong"],"categories":null,"content":"","date":1671494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683609864,"objectID":"1f2213cfc2685eaea27bdc173a794423","permalink":"https://about.blaok.me/publication/cacm22-democratizing/","publishdate":"2022-12-20T00:00:00Z","relpermalink":"/publication/cacm22-democratizing/","section":"publication","summary":"Creating a programming environment and compilation flow that empowers programmers to create their own DSAs efficiently and affordably on FPGAs.","tags":null,"title":"Democratizing Domain-Specific Computing","type":"publication"},{"authors":["Young-kyu Choi","**Yuze Chi**","Jason Lau","Jason Cong"],"categories":null,"content":"","date":1666310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683609471,"objectID":"efcfd29a6b24fc45b9f29cd28e6693b0","permalink":"https://about.blaok.me/publication/tcad22-taro/","publishdate":"2022-10-21T00:00:00Z","relpermalink":"/publication/tcad22-taro/","section":"publication","summary":"Streaming applications have become one of the key application domains for high-level synthesis (HLS) tools. For a streaming application, there is a potential to simplify the control logic by regulating each task with a stream of input and output data. This is called free-running optimization. But it is difficult to understand when such optimization can be applied without changing the functionality of the original design. Moreover, it takes a large effort to manually apply the optimization across legacy codes. In this paper, we present TARO framework which automatically applies the free-running optimization on HLS-based streaming applications. TARO simplifies the control logic without degrading the clock frequency or the performance. Experiments on Alveo U250 shows that we can obtain an average of 16% LUT and 45% FF reduction for streaming-based systolic array designs.","tags":null,"title":"TARO: Automatic Optimization for Free-Running Kernels in FPGA High-Level Synthesis","type":"publication"},{"authors":["Linghao Song","**Yuze Chi**","Licheng Guo","Jason Cong"],"categories":null,"content":"","date":1657411200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683608737,"objectID":"d39f244b6ba7f0c5cdc8002150dd931b","permalink":"https://about.blaok.me/publication/dac22-serpens/","publishdate":"2022-07-10T00:00:00Z","relpermalink":"/publication/dac22-serpens/","section":"publication","summary":"Sparse matrix-vector multiplication (SpMV) multiplies a sparse matrix with a dense vector. SpMV plays a crucial role in many applications, from graph analytics to deep learning. The random memory accesses of the sparse matrix make accelerator design challenging. However, high bandwidth memory (HBM) based FPGAs are a good fit for designing accelerators for SpMV. In this paper, we present Serpens, an HBM based accelerator for general-purpose SpMV, which features memory-centric processing engines and index coalescing to support the efficient processing of arbitrary SpMVs. From the evaluation of twelve large-size matrices, Serpens is 1.91x and 1.76x better in terms of geomean throughput than the latest accelerators GraphLiLy and Sextans, respectively. We also evaluate 2,519 SuiteSparse matrices, and Serpens achieves 2.10x higher throughput than a K80 GPU. For the energy/bandwidth efficiency, Serpens is 1.71x/1.99x, 1.90x/2.69x, and 6.25x/4.06x better compared with GraphLily, Sextans, and K80, respectively. After scaling up to 24 HBM channels, Serpens achieves up to 60.55 GFLOP/s (30,204 MTEPS) and up to 3.79x over GraphLily. The code is available at https://github.com/UCLA-VAST/Serpens.","tags":null,"title":"Serpens: A High Bandwidth Memory Based Accelerator for General-Purpose Sparse Matrix-Vector Multiplication","type":"publication"},{"authors":["Linghao Song","**Yuze Chi**","Jason Cong"],"categories":null,"content":"","date":1653264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683608414,"objectID":"dc900f20b9bb5a882e48a29072904d69","permalink":"https://about.blaok.me/publication/icassp22-pyxis/","publishdate":"2022-05-23T00:00:00Z","relpermalink":"/publication/icassp22-pyxis/","section":"publication","summary":"Customized accelerators provide gains of performance and efficiency in specific domains of applications. Sparse data structures and/or representations exist in a wide range of applications. However, it is challenging to design accelerators for sparse applications because no architecture or performance-level analytic models are able to fully capture the spectrum of the sparse data. Accelerator researchers rely on real execution to get precise feedback for their designs. In this work, we present PYXIS, a performance dataset for customized accelerators on sparse data. PYXIS collects accelerator designs and real execution performance statistics. Currently, there are 73.8 K instances in PYXIS. PYXIS is open-source, and we are constantly growing PYXIS with new accelerator designs and performance statistics. PYXIS can be a benefit to researchers in the fields of accelerator, architecture, performance, algorithm and many related topics.","tags":null,"title":"PYXIS: An Open-Source Performance Dataset Of Sparse Accelerators","type":"publication"},{"authors":["Atefeh Sohrabizadeh","**Yuze Chi**","Jason Cong"],"categories":null,"content":"","date":1650585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683608105,"objectID":"4e3604380907c48c7626de02b41a3925","permalink":"https://about.blaok.me/publication/cicc22-streamgcn/","publishdate":"2022-04-22T00:00:00Z","relpermalink":"/publication/cicc22-streamgcn/","section":"publication","summary":"While there have been many studies on hardware acceleration for deep learning on images, there has been a rather limited focus on accelerating deep learning applications involving graphs. The unique characteristics of graphs, such as the irregular memory access and dynamic parallelism, impose several challenges when the algorithm is mapped to a CPU or GPU. To address these challenges while exploiting all the available sparsity, we propose a flexible architecture called StreamGCN for accelerating Graph Convolutional Networks (GCN), the core computation unit in deep learning algorithms on graphs. The architecture is specialized for streaming processing of many small graphs for graph search and similarity computation. The experimental results demonstrate that StreamGCN can deliver a high speedup compared to a multi-core CPU and a GPU implementation, showing the efficiency of our design.","tags":null,"title":"StreamGCN: Accelerating Graph Convolutional Networks with Streaming Processing","type":"publication"},{"authors":["**Yuze Chi**","Licheng Guo","Jason Cong"],"categories":null,"content":"","date":1645920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670566198,"objectID":"b34b142f5e30f85c2df400ed135783ad","permalink":"https://about.blaok.me/publication/splag/","publishdate":"2022-02-27T00:00:00Z","relpermalink":"/publication/splag/","section":"publication","summary":"The single-source shortest path (SSSP) problem is one of the most important and well-studied graph problems widely used in many application domains, such as road navigation, neural image reconstruction, and social network analysis. Although we have known various SSSP algorithms for decades, implementing one for large-scale power-law graphs efficiently is still highly challenging today, because ① a work-efficient SSSP algorithm requires priority-order traversal of graph data, ② the priority queue needs to be scalable both in throughput and capacity, and ③ priority-order traversal requires extensive random memory accesses on graph data.In this paper, we present SPLAG to accelerate SSSP for power-law graphs on FPGAs. SPLAG uses a coarse-grained priority queue (CGPQ) to enable high-throughput priority-order graph traversal with a large frontier. To mitigate the high-volume random accesses, SPLAG employs a customized vertex cache (CVC) to reduce off-chip memory access and improve the throughput to read and update vertex data. Experimental results on various synthetic and real-world datasets show up to a 4.9× speedup over state-of-the-art SSSP accelerators, a 2.6× speedup over 32-thread CPU running at 4.4 GHz, and a 0.9× speedup over an A100 GPU that has 4.1× power budget and 3.4× HBM bandwidth. Such a high performance would place SPLAG in the 14th position of the Graph 500 benchmark for data intensive applications (the highest using a single FPGA) with only a 45 W power budget. SPLAG is written in high-level synthesis C++ and is fully parameterized, which means it can be easily ported to various different FPGAs with different configurations. SPLAG is open-source at https://github.com/UCLA-VAST/splag.","tags":null,"title":"Accelerating SSSP for Power-Law Graphs","type":"publication"},{"authors":["Licheng Guo","Pongstorn Maidee","Yun Zhou","Chris Lavin","Jie Wang","**Yuze Chi**","Weikang Qiao","Alireza Kaviani","Zhiru Zhang","Jason Cong"],"categories":null,"content":"","date":1645920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650939555,"objectID":"21be0547b46784199d35c7db91c1ceb4","permalink":"https://about.blaok.me/publication/rapidstream/","publishdate":"2022-02-27T00:00:00Z","relpermalink":"/publication/rapidstream/","section":"publication","summary":"FPGAs require a much longer compilation cycle than conventional computing platforms like CPUs. In this paper, we shorten the overall compilation time by co-optimizing the HLS compilation (C-to-RTL) and the back-end physical implementation (RTL-to-bitstream). We propose a split compilation approach based on the pipelining flexibility at the HLS level, which allows us to partition designs for parallel placement and routing then stitch the separate partitions together. We outline a number of technical challenges and address them by breaking the conventional boundaries between different stages of the traditional FPGA tool flow and reorganizing them to achieve a fast end-to-end compilation.  Our research produces RapidStream, a parallelized and physical-integrated compilation framework that takes in an HLS dataflow program in C/C++ and generates a fully placed and routed implementation. When tested on the Xilinx U250 FPGA with a set of realistic HLS designs, RapidStream achieves a 5-7X reduction in compile time and up to 1.3X increase in frequency when compared to a commercial-off-the-shelf toolchain. In addition, we provide preliminary results using a customized open-source router to reduce the compile time up to an order of magnitude in the cases with lower performance requirements. The tool is open-sourced at https://github.com/Licheng-Guo/RapidStream.","tags":null,"title":"RapidStream: Parallel Physical Implementation of FPGA HLS Designs","type":"publication"},{"authors":["Linghao Song","**Yuze Chi**","Atefeh Sohrabizadeh","Young-kyu Choi","Jason Lau","Jason Cong"],"categories":null,"content":"","date":1645920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639726918,"objectID":"98c6a106c9321c41def8db1a80555c00","permalink":"https://about.blaok.me/publication/sextans/","publishdate":"2022-02-27T00:00:00Z","relpermalink":"/publication/sextans/","section":"publication","summary":"Sparse-Matrix Dense-Matrix multiplication (SpMM) is the key operator for a wide range of applications including scientific computing, graph processing, and deep learning. Architecting accelerators for SpMM is faced with three challenges – (1) the random memory accessing and unbalanced load in processing because of random distribution of elements in sparse matrices, (2) inefficient data handling of the large matrices which can not be fit on-chip, and (3) a non-general-purpose accelerator design where one accelerator can only process a fixed-size problem.In this paper, we present Sextans, an accelerator for general-purpose SpMM processing. Sextans accelerator features (1) fast random access using on-chip memory, (2) streaming access to off-chip large matrices, (3) PE-aware non-zero scheduling for balanced workload with an II=1 pipeline, and (4) hardware flexibility to enable prototyping the hardware once to support SpMMs of different size as a general-purpose accelerator. We leverage high bandwidth memory (HBM) for the efficient accessing of both sparse and dense matrices. In the evaluation, we present an FPGA prototype Sextans which is executable on a Xilinx U280 HBM FPGA board and a projected prototype Sextans-P with higher bandwidth competitive to V100 and more frequency optimization. We conduct a comprehensive evaluation on 1,400 SpMMs on a wide range of sparse matrices including 50 matrices from SNAP and 150 from SuiteSparse. We compare Sextans with NVIDIA K80 and V100 GPUs. Sextans achieves a 2.50x geomean speedup over K80 GPU, and Sextans-P achieves a 1.14x geomean speedup over V100 GPU (4.94x over K80). The code is available at https://github.com/linghaosong/Sextans.","tags":null,"title":"Sextans: A Streaming Accelerator for General-Purpose Sparse-Matrix Dense-Matrix Multiplication","type":"publication"},{"authors":["**Yuze Chi**"],"categories":null,"content":"","date":1639094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683611365,"objectID":"1d22df943bb32ebec8b16981bee0bdbf","permalink":"https://about.blaok.me/publication/dissertation/","publishdate":"2021-12-10T00:00:00Z","relpermalink":"/publication/dissertation/","section":"publication","summary":"As we witness the breakdown of Dennard scaling, we can no longer get faster computers by shrinking transistors without increasing power density. Yet, the amount of data to be processed has never stopped growing. The limited power budget builds a “power wall” between the ever-increasing demand for computation and the available computer hardware, which forces computer scientists to seek not only performant, but also power efficient computation solutions, especially in data centers. Moreover, the wide performance gap between the computation units and the memory builds a “memory wall” and limits performance from another dimension. In the past decade, field-programmable gate arrays (FPGAs) have been rapidly adopted in data centers, thanks to their low power consumption and the reprogrammability that assemble highly power-efficient accelerators for memory-bound applications. Meanwhile, C-based high-level synthesis (HLS) has been growing as the FPGA acceleration market grows, bringing “hard-to-program” FPGA accelerators to a broader community in many application domains. However, to create efficient customized accelerators, FPGA-related expertise is still required for the domain experts when they write HLS C. To make it worse, even for experienced FPGA programmers, C-based HLS is often less productive compared with higher-level software languages, especially when an application cannot be easily programmed using the compiler directives designed for data-parallel programs. This dissertation aims to address these two issues for domain-specific customizable accelerators for memory-bound applications with both regular and irregular memory access patterns. For memory-bound applications with regular memory accesses, we select stencil application as a representative for their complex data dependency that is challenging to optimize. We present SODA (Stencil with Optimized Dataflow Architecture) as a domain-specific compiler framework for FPGA accelerators. We show that by adopting theoretical analysis, model-driven design-space exploration, and domain-specific languages, programmers without FPGA expertise can build highly efficient stencil accelerators that outperform multi-thread CPUs by up to 3.3× with the memory bandwidth utilization improved by 1.65× on average. For memory-bound applications with irregular memory accesses, we select graph applications as a representative for their widespread presentation in various application domains. We first present TAPA (TAsk-PArallel) as a language extension to HLS, showing that convenient programming interfaces, universal software simulation, and hierarchical code generation can greatly improve productivity for task-parallel programs and reduce programmers’ burden. We then extend our effort to support dynamically scheduled memory accesses to cover more applications and further improve productivity. Finally, we show with two case studies from real-world graph applications, i.e., single-source shortest path for neural image reconstruction and graph convolutional neural network for learning on graph structure, that customizable accelerators can achieve up to 4.9× speedup over state-of-the-art FPGA accelerators and 2.6× speedup over state-of-the-art multi-thread CPU implementation.","tags":null,"title":"Design Automation and Optimization for Memory-Bound Application Accelerators","type":"publication"},{"authors":["Karl Marrett","Muye Zhu","**Yuze Chi**","Chris Choi","Zhe Chen","Hong-Wei Dong","Chang Sin Park","X. William Yang","Jason Cong"],"categories":null,"content":"","date":1639008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683611732,"objectID":"d94d93bf866b987748c262d5b181f9b4","permalink":"https://about.blaok.me/publication/recut/","publishdate":"2021-12-09T00:00:00Z","relpermalink":"/publication/recut/","section":"publication","summary":"Advancement in modern neuroscience is bottlenecked by neural reconstruction, a process that extracts 3D neuron morphology (typically in tree structures) from image volumes at the scale of hundreds of GBs. We introduce Recut, an automated and accelerated neural reconstruction pipeline, which provides a unified, and domain specific sparse data representation with 79× reduction in the memory footprint. Recut’s reconstruction can process 111 Kneurons/day or 79 TB/day on a 24-core workstation, placing the throughput bottleneck back on microscopic imaging time. Recut allows the full brain of a mouse to be processed in memory on a single server, at 89.5× higher throughput over existing I/O-bounded methods. Recut is also the first fully parallelized end-to-end automated reconstruction pipeline for light microscopy, yielding tree morphologies closer to ground truth than the state-of-the-art while removing involved manual steps and disk I/O overheads. We also optimized pipeline stages to linear algorithmic complexity for scalability in dense settings and allow the most timing-critical stages to optionally run on accelerated hardware.","tags":null,"title":"Recut: a Concurrent Framework for Sparse Reconstruction of Neuronal Morphology","type":"publication"},{"authors":["Atefeh Sohrabizadeh","**Yuze Chi**","Jason Cong"],"categories":null,"content":"","date":1636502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683611912,"objectID":"29b91cd9549eec28ebf8ff368fa527b6","permalink":"https://about.blaok.me/publication/spa-gcn/","publishdate":"2021-11-10T00:00:00Z","relpermalink":"/publication/spa-gcn/","section":"publication","summary":"While there have been many studies on hardware acceleration for deep learning on images, there has been a rather limited focus on accelerating deep learning applications involving graphs. The unique characteristics of graphs, such as the irregular memory access and dynamic parallelism, impose several challenges when the algorithm is mapped to a CPU or GPU. To address these challenges while exploiting all the available sparsity, we propose a flexible architecture called SPA-GCN for accelerating Graph Convolutional Networks (GCN), the core computation unit in deep learning algorithms on graphs. The architecture is specialized for dealing with many small graphs since the graph size has a significant impact on design considerations. In this context, we use SimGNN, a neural-network-based graph matching algorithm, as a case study to demonstrate the effectiveness of our architecture. The experimental results demonstrate that SPA-GCN can deliver a high speedup compared to a multi-core CPU implementation and a GPU implementation, showing the efficiency of our design.","tags":null,"title":"SPA-GCN: Efficient and Flexible GCN Accelerator with an Application for Graph Similarity Computation","type":"publication"},{"authors":["**Yuze Chi**","Licheng Guo","Jason Lau","Young-kyu Choi","Jie Wang","Jason Cong"],"categories":null,"content":"","date":1620691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621123521,"objectID":"77d8c6c9546370b2d14b36cb8bab0fc0","permalink":"https://about.blaok.me/publication/tapa/","publishdate":"2021-05-11T00:00:00Z","relpermalink":"/publication/tapa/","section":"publication","summary":"C/C++/OpenCL-based high-level synthesis (HLS) becomes more and more popular for field-programmable gate array (FPGA) accelerators in many application domains in recent years, thanks to its competitive quality of results (QoR) and short development cycles compared with the traditional register-transfer level design approach. Yet, limited by the sequential C semantics, it remains challenging to adopt the same highly productive high-level programming approach in many other application domains, where coarse-grained tasks run in parallel and communicate with each other at a fine-grained level. While current HLS tools do support task-parallel programs, the productivity is greatly limited ① in the code development cycle due to the poor programmability, ② in the correctness verification cycle due to restricted software simulation, and ③ in the QoR tuning cycle due to slow code generation. Such limited productivity often defeats the purpose of HLS and hinder programmers from adopting HLS for task-parallel FPGA accelerators.In this paper, we extend the HLS C++ language and present a fully automated framework with programmer-friendly interfaces, unconstrained software simulation, and fast hierarchical code generation to overcome these limitations and demonstrate how task-parallel programs can be productively supported in HLS. Experimental results based on a wide range of real-world task-parallel programs show that, on average, the lines of kernel and host code are reduced by 22% and 51%, respectively, which considerably improves the programmability. The correctness verification and the iterative QoR tuning cycles are both greatly shortened by 3.2× and 6.8×, respectively. Our work is open-source at https://github.com/UCLA-VAST/tapa/.","tags":null,"title":"Extending High-Level Synthesis for Task-Parallel Programs","type":"publication"},{"authors":["Licheng Guo","**Yuze Chi**","Jie Wang","Jason Lau","Weikang Qiao","Ecenur Ustun","Zhiru Zhang","Jason Cong"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650939555,"objectID":"0633d97b4cdcef04aedc43d8f557e3a1","permalink":"https://about.blaok.me/publication/autobridge/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/autobridge/","section":"publication","summary":"Despite an increasing adoption of high-level synthesis (HLS) for its design productivity advantages, there remains a significant gap in the achievable clock frequency between an HLS-generated design and a handcrafted RTL one. A key factor that limits the timing quality of the HLS outputs is the difficulty in accurately estimating the interconnect delay at the HLS level. Unfortunately, this problem becomes even worse when large HLS designs are implemented on the latest multi-die FPGAs, where die-crossing interconnects incur a high delay penalty.To tackle this challenge, we propose AutoBridge, an automated framework that couples a coarse-grained floorplanning step with pipelining during HLS compilation. First, our approach provides HLS with a view on the global physical layout of the design, allowing HLS to more easily identify and pipeline the long wires, especially those crossing the die boundaries. Second, by exploiting the flexibility of HLS pipelining, the floorplanner is able to distribute the design logic across multiple dies on the FPGA device without degrading clock frequency. This prevents the placer from aggressively packing the logic on a single die which often results in local routing congestion that eventually degrades timing. Since pipelining may introduce additional latency, we further present analysis and algorithms to ensure the added latency will not compromise the overall throughput.AutoBridge can be integrated into the existing CAD toolflow for Xilinx FPGAs. In our experiments with a total of 43 design configurations, we improve the average frequency from 147 MHz to 297MHz (a 102% improvement) with no loss of throughput and a negligible change in resource utilization. Notably, in 16 experiments we make the originally unroutable designs achieve 274 MHz on average. The tool is available at https://github.com/Licheng-Guo/AutoBridge.","tags":null,"title":"AutoBridge: Coupling Coarse-Grained Floorplanning and Pipelining for High-Frequency HLS Design on Multi-Die FPGAs","type":"publication"},{"authors":["Young-kyu Choi","**Yuze Chi**","Weikang Qiao","Nikola Samardzic","Jason Cong"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614718828,"objectID":"939363135ae9b423b970adba9ec1b1ed","permalink":"https://about.blaok.me/publication/hbm-connect/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/hbm-connect/","section":"publication","summary":"With the recent release of High Bandwidth Memory (HBM) based FPGA boards, developers can now exploit unprecedented external memory bandwidth. This allows more memory-bounded applications to benefit from FPGA acceleration. However, fully utilizing the available bandwidth may not be an easy task. If an application requires multiple processing elements to access multiple HBM channels, we observed a significant drop in the effective bandwidth. The existing high-level synthesis (HLS) programming environment had limitation in producing an efficient communication architecture. In order to solve this problem, we propose HBM Connect, a high-performance customized interconnect for FPGA HBM board. Novel HLS-based optimization techniques are introduced to increase the throughput of AXI bus masters and switching elements. We also present a high-performance customized crossbar that may replace the built-in crossbar. The effectiveness of HBM Connect is demonstrated using Xilinx’s Alveo U280 HBM board. Based on bucket sort and merge sort case studies, we explore several design spaces and find the design point with the best resource-performance trade-off. The result shows that HBM Connect improves the resource-performance metrics by 6.5X–211X.","tags":null,"title":"HBM Connect: High-Performance HLS Interconnect for FPGA HBM","type":"publication"},{"authors":["Young-kyu Choi","**Yuze Chi**","Jie Wang","Licheng Guo","Jason Cong"],"categories":null,"content":"","date":1602460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626135915,"objectID":"60cde95aab58fd04873fa08fb648c471","permalink":"https://about.blaok.me/publication/hbm-bench/","publishdate":"2020-10-12T00:00:00Z","relpermalink":"/publication/hbm-bench/","section":"publication","summary":"With the recent release of High Bandwidth Memory (HBM) based FPGA boards, developers can now exploit unprecedented external memory bandwidth. This allows more memory-bounded applications to benefit from FPGA acceleration. However, we found that it is not easy to fully utilize the available bandwidth when developing some applications with high-level synthesis (HLS) tools. This is due to the limitation of existing HLS tools when accessing HBM board’s large number of independent external memory channels. In this paper, we measure the performance of three recent representative HBM FPGA boards (Intel’s Stratix 10 MX and Xilinx’s Alveo U50/U280 boards) with microbenchmarks and analyze the HLS overhead. Next, we propose HLS-based optimization techniques to improve the effective bandwidth when a PE accesses multiple HBM channels or multiple PEs access an HBM channel. Our experiment demonstrates that the effective bandwidth improves by 2.4X-3.8X. We also provide a list of insights for future improvement of the HBM FPGA HLS design flow.","tags":null,"title":"When HLS Meets FPGA HBM: Benchmarking and Bandwidth Optimization","type":"publication"},{"authors":["**Yuze Chi**","Jason Cong"],"categories":null,"content":"","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608444581,"objectID":"6caa9d3ac7b26a5f80e79ac2a0c5406c","permalink":"https://about.blaok.me/publication/soda-cr/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/publication/soda-cr/","section":"publication","summary":"Stencil kernel is an important type of kernel used extensively in many application domains. Over the years, researchers have been studying the optimizations on parallelization, communication reuse, and computation reuse for various target platforms. However, challenges still exist, especially on the computation reuse problem for accelerators, due to the lack of complete design-space exploration and effective design-space pruning. In this paper, we present solutions to the above challenges for a wide range of stencil kernels (i.e., stencil with reduction operations), where the computation reuse patterns are extremely flexible due to the commutative and associative properties. We formally define the complete design space, based on which we present a provably optimal dynamic programming algorithm and a heuristic beam search algorithm that provides near-optimal solutions under an architecture-aware model.  Experimental results show that for synthesizing stencil kernels to FPGAs, compared with state-of-the-art stencil compiler without computation reuse capability, our proposed algorithm can reduce the look-up table (LUT) and digital signal processor (DSP) usage by 58.1% and 54.6% on average respectively, which leads to an average speedup of 2.3× for compute-intensive kernels, outperforming the latest CPU/GPU results.","tags":null,"title":"Exploiting Computation Reuse for Stencil Accelerators","type":"publication"},{"authors":["Licheng Guo","Jason Lau","**Yuze Chi**","Jie Wang","Cody Hao Yu","Zhe Chen","Zhiru Zhang","Jason Cong"],"categories":null,"content":"","date":1595462400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650939555,"objectID":"5c5318483664c216d33f9e5f86a8e09b","permalink":"https://about.blaok.me/publication/hls-timing/","publishdate":"2020-07-23T00:00:00Z","relpermalink":"/publication/hls-timing/","section":"publication","summary":"Designs generated by high-level synthesis (HLS) tools typically achieve a lower frequency compared to manual RTL designs. In this work, we study the timing issues in a diverse set of realistic and complex FPGA HLS designs. (1) We observe that in almost all cases the frequency degradation is caused by the broadcast structures generated by the HLS compiler. (2) We classify three major types of broadcasts in HLS-generated designs, including high-fanout data signals, pipeline flow control signals and synchronization signals for concurrent modules. (3) We reveal a number of limitations of the current HLS tools that result in those broadcast-related timing issues. (4) We propose a set of effective yet easy-to-implement approaches, including broadcast-aware scheduling, synchronization pruning, and skid-buffer-based flow control. Our experimental results show that our methods can improve the maximum frequency of a set of nine representative HLS benchmarks by 53% on average. In some cases, the frequency gain is more than 100 MHz.","tags":null,"title":"Analysis and Optimization of the Implicit Broadcasts in FPGA HLS to Improve Maximum Frequency","type":"publication"},{"authors":["Jiajie Li","**Yuze Chi**","Jason Cong"],"categories":null,"content":"","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582581653,"objectID":"96f95486a78932196cb83be1bb716339","permalink":"https://about.blaok.me/publication/heterohalide/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/publication/heterohalide/","section":"publication","summary":"The domain-specific language (DSL) for image processing, Halide, has generated a lot of interest because of its capability of decoupling algorithms from schedules that allow programmers to search for optimized mappings targeting CPU and GPU. Unfortunately, while the Halide community has been growing rapidly, there is currently no way to easily map the vast number of Halide programs to efficient FPGA accelerators. To tackle this challenge, we propose HeteroHalide, an end-to-end system for compiling Halide programs to FPGA accelerators. This system makes use of both algorithm and scheduling information specified in a Halide program. Compared to the existing approaches, flow provided by HeteroHalide is significantly simplified, as it only requires moderate modifications for Halide programs on the scheduling part to be applicable to FPGAs. For part of the compilation flow, and to act as the intermediate representation (IR) of HeteroHalide, we choose HeteroCL, a heterogeneous programming infrastructure which supports multiple implementation backends (such as systolic arrays and stencil implementations).  By using HeteroCL, HeteroHalide can generate efficient accelerators by choosing different backends according to the application. The performance evaluation compares the accelerator generated by HeteroHalide with multi-core CPU and an existing Halide-HLS compiler. As a result, HeteroHalide achieves 4.15× speedup on average over 28 CPU cores, and 2 ~ 4× throughput improvement compared with the existing Halide-HLS compiler","tags":null,"title":"HeteroHalide: From Image Processing DSL to Efficient FPGA Acceleration","type":"publication"},{"authors":["Young-kyu Choi","**Yuze Chi**","Jie Wang","Jason Cong"],"categories":null,"content":"","date":1580342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635389327,"objectID":"f4242cc0307ded21f8a143d56ac30a72","permalink":"https://about.blaok.me/publication/flash-journal/","publishdate":"2020-01-30T00:00:00Z","relpermalink":"/publication/flash-journal/","section":"publication","summary":"A large semantic gap between a high-level synthesis (HLS) design and a low-level RTL simulation environment often creates a barrier for those who are not FPGA experts. Moreover, such a low-level simulation takes a long time to complete. Software HLS simulators can help bridge this gap and accelerate the simulation process; but their shortcoming is that they do not provide performance estimation. To make matters worse, we found that the current FPGA HLS commercial software simulators sometimes produce incorrect results. In order to solve these performance estimation and correctness problems while maintaining the high speed of software simulators, this paper proposes a new HLS simulation flow named FLASH. The main idea behind the proposed flow is to extract scheduling information from the HLS tool and automatically construct an equivalent cycle-accurate simulation model while preserving C semantics. Experimental results show that FLASH runs three orders of magnitude faster than the RTL simulation.","tags":null,"title":"FLASH: Fast, ParalleL, and Accurate Simulator for HLS","type":"publication"},{"authors":["Yi-Hsiang Lai","**Yuze Chi**","Yuwei Hu","Jie Wang","Cody Hao Yu","Yuan Zhou","Jason Cong","Zhiru Zhang"],"categories":null,"content":"","date":1550966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650939555,"objectID":"4b2061c6474189a1a600272bca3e1821","permalink":"https://about.blaok.me/publication/heterocl/","publishdate":"2019-02-24T00:00:00Z","relpermalink":"/publication/heterocl/","section":"publication","summary":"With the pursuit of improving compute performance under strict power constraints, there is an increasing need for deploying applications to heterogeneous hardware architectures with accelerators, such as GPUs and FPGAs. However, although these heterogeneous computing platforms are becoming widely available, they are very difficult to program especially with FPGAs. As a result, the use of such platforms has been limited to a small subset of programmers with specialized hardware knowledge.  To tackle this challenge, we introduce HeteroCL, a programming infrastructure composed of a Python-based domain-specific language (DSL) and an FPGA-targeted compilation flow. The HeteroCL DSL provides a clean programming abstraction that decouples algorithm specification from three important types of hardware customization in compute, data types, and memory architectures. HeteroCL further captures the interdependence among these different customization techniques, allowing programmers to explore various performance/area/accuracy trade-offs in a systematic and productive manner. In addition, our framework produces highly efficient hardware implementations for a variety of popular workloads by targeting spatial architecture templates such as systolic arrays and stencil with dataflow architectures. Experimental results show that HeteroCL allows programmers to explore the design space efficiently in both performance and accuracy by combining different types of hardware customization and targeting spatial architectures, while keeping the algorithm code intact.","tags":null,"title":"HeteroCL: A Multi-Paradigm Programming Infrastructure for Software-Defined Reconfigurable Computing","type":"publication"},{"authors":["**Yuze Chi**","Young-kyu Choi","Jason Cong","Jie Wang"],"categories":null,"content":"","date":1550966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551376195,"objectID":"e4b8ca9695d4bcfe4422ed327dbae191","permalink":"https://about.blaok.me/publication/flash/","publishdate":"2019-02-24T00:00:00Z","relpermalink":"/publication/flash/","section":"publication","summary":"A large semantic gap between the high-level synthesis (HLS) design and the low-level (on-board or RTL) simulation environment often creates a barrier for those who are not FPGA experts. Moreover, such low-level simulation takes a long time to complete. Software-based HLS simulators can help bridge this gap and accelerate the simulation process; however, we found that the current FPGA HLS commercial software simulators sometimes produce incorrect results. In order to solve this correctness issue while maintaining the high speed of a software-based simulator, this paper proposes a new HLS simulation flow named FLASH. The main idea behind the proposed flow is to extract the scheduling information from the HLS tool and automatically construct an equivalent cycle-accurate simulation model while preserving C semantics. Experimental results show that FLASH runs three orders of magnitude faster than the RTL simulation.","tags":null,"title":"Rapid Cycle-Accurate Simulator for High-Level Synthesis","type":"publication"},{"authors":["**Yuze Chi**","Jason Cong","Peng Wei","Peipei Zhou"],"categories":null,"content":"","date":1541376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588909599,"objectID":"703657b0af05ab2bdf57853c59c06d3f","permalink":"https://about.blaok.me/publication/soda/","publishdate":"2018-11-05T00:00:00Z","relpermalink":"/publication/soda/","section":"publication","summary":"Stencil computation is one of the most important kernels in many application domains such as image processing, solving partial diferential equations, and cellular automata. Many of the stencil kernels are complex, usually consist of multiple stages or iterations, and are often computation-bounded. Such kernels are often off-loaded to FPGAs to take advantages of the efficiency of dedicated hardware. However, implementing such complex kernels efficiently is not trivial, due to complicated data dependencies, difficulties of programming FPGAs with RTL, as well as large design space. In this paper we present SODA, an automated framework for implementing **S**tencil algorithms with **O**ptimized **D**atalow **A**rchitecture on FPGAs. The SODA microarchitecture minimizes the on-chip reuse bufer size required by full data reuse and provides flexible and scalable fine-grained parallelism. The SODA automation framework takes high-level user input and generates efficient, high-frequency datalow implementation. This significantly reduces the difficulty of programming FPGAs efficiently for stencil algorithms. The SODA design-space exploration framework models the resource constraints and searches for the performance-optimized coniguration with accurate models for post-synthesis resource utilization and on-board execution throughput. Experimental results from on-board execution using a wide range of benchmarks show up to 3.28x speed up over 24-thread CPU and our fully automated framework achieves better performance compared with manually designed state-of-the-art FPGA accelerators.","tags":null,"title":"SODA: Stencil with Optimized Dataflow Architecture","type":"publication"},{"authors":["Guohao Dai","Tianhao Huang","**Yuze Chi**","Jishen Zhao","Guangyu Sun","Yongpan Liu","Yu Wang","Yuan Xie","Huazhong Yang"],"categories":null,"content":"","date":1522368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541806895,"objectID":"a82ede1c1a59a66da3fbe1a5759f6852","permalink":"https://about.blaok.me/publication/graphh/","publishdate":"2018-03-30T00:00:00Z","relpermalink":"/publication/graphh/","section":"publication","summary":"Large-scale graph processing requires the high bandwidth of data access. However, as graph computing continues to scale, it becomes increasingly challenging to achieve a high bandwidth on generic computing architectures. The primary reasons include: the random access pattern causing local bandwidth degradation, the poor locality leading to unpredictable global data access, heavy conflicts on updating the same vertex, and unbalanced workloads across processing units. Processing-in-memory has been explored as a promising solution to providing high bandwidth, yet open questions of graph processing on PIM devices remain in: (1) How to design hardware specializations and the interconnection scheme to fully utilize bandwidth of PIM devices and ensure locality; (2) How to allocate data and schedule processing flow to avoid conflicts and balance workloads. In this paper, we propose GraphH, a PIM architecture for graph processing on the Hybrid Memory Cube array, to tackle all four problems mentioned above. From the architecture perspective, we integrate SRAM-based On-chip Vertex Buffers to eliminate local bandwidth degradation; We also introduce Reconfigurable Double-Mesh Connection to provide high global bandwidth. From the algorithm perspective, partitioning and scheduling methods like Index Mapping Interval-Block and Round Interval Pair are introduced to GraphH, thus workloads are balanced and conflicts are avoided. Two optimization methods are further introduced to reduce synchronization overhead and reuse on-chip data. The experimental results on graphs with billions of edges demonstrate that GraphH outperforms DDR-based graph processing systems by up to two orders of magnitude and 5.12x speedup against the previous PIM design iscagraph.","tags":null,"title":"GraphH: A Processing-in-Memory Architecture for Large-scale Graph Processing","type":"publication"},{"authors":["**Yuze Chi**","Peipei Zhou","Jason Cong"],"categories":null,"content":"","date":1519603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541806895,"objectID":"b0ec85a18ce4f29ed3ad90d56c16b461","permalink":"https://about.blaok.me/publication/supo/","publishdate":"2018-02-26T00:00:00Z","relpermalink":"/publication/supo/","section":"publication","summary":"Stencil computation is one of the most important kernels for many applications such as image processing, solving partial differential equations, and cellular automata. Nevertheless, implementing a high throughput stencil kernel is not trivial due to its nature of high memory access load and low operational intensity. In this work we adopt data reuse and fine-grained parallelism and present an optimal microarchitecture for stencil computation. The data reuse line buffers not only fully utilize the external memory bandwidth and fully reuse the input data, they also minimize the size of data reuse buffer given the number of fine-grained parallelized and fully pipelined PEs. With the proposed microarchitecture, the number of PEs can be increased to saturate all available off-chip memory bandwidth. We implement this microarchitecture with a high-level synthesis (HLS) based template instead of register transfer level (RTL) specifications, which provides great programmability. To guide the system design, we propose a performance model in addition to detailed model evaluation and optimization analysis. Experimental results from on-board execution show that our design can provide an average of 6.5x speedup over line buffer-only design with only 2.4x resource overhead. Compared with loop transformation-only design, our design can implement a fully pipelined accelerator for applications that cannot be implemented with loop transformation-only due to its high memory conflict and low design flexibility. Furthermore, our FPGA implementation provides 83% throughput of a 14-core CPU with 4x energy-efficiency.","tags":null,"title":"An Optimal Microarchitecture for Stencil Computation with Data Reuse and Fine-Grained Parallelism","type":"publication"},{"authors":["Guohao Dai","Tianhao Huang","**Yuze Chi**","Ningyi Xu","Yu Wang","Huazhong Yang"],"categories":null,"content":"","date":1487721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541806895,"objectID":"010ace9ba680d173aa14b0965b6e4d51","permalink":"https://about.blaok.me/publication/foregraph/","publishdate":"2017-02-22T00:00:00Z","relpermalink":"/publication/foregraph/","section":"publication","summary":"The performance of large-scale graph processing suffers from challenges including poor locality, lack of scalability, random access pattern, and heavy data conflicts. Some characteristics of FPGA make it a promising solution to accelerate various applications. For example, on-chip block RAMs can provide high throughput for random data access. However, large-scale processing on a single FPGA chip is constrained by limited on-chip memory resources and off-chip bandwidth. Using a multi-FPGA architecture may alleviate these problems to some extent, while the data partitioning and communication schemes should be considered to ensure the locality and reduce data conflicts. In this paper, we propose ForeGraph, a large-scale graph processing framework based on the multi-FPGA architecture. In ForeGraph, each FPGA board only stores a partition of the entire graph in off-chip memory. Communication over partitions is reduced. Vertices and edges are sequentially loaded onto the FPGA chip and processed. Under our scheduling scheme, each FPGA chip performs graph processing in parallel without conflicts. We also analyze the impact of system parameters on the performance of ForeGraph. Our experimental results on Xilinx Virtex UltraScale XCVU190 chip show ForeGraph outperforms state-ofthe-art FPGA-based large-scale graph processing systems by 4.54x when executing PageRank on the Twitter graph (1.4 billion edges). The average throughput is over 900 MTEPS in our design and 2.03x larger than previous work.","tags":null,"title":"ForeGraph: Exploring Large-scale Graph Processing on Multi-FPGA Architecture","type":"publication"},{"authors":["Guohao Dai","**Yuze Chi**","Yu Wang","Huazhong Yang"],"categories":null,"content":"","date":1466467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541806895,"objectID":"b2be6579fa499347641b983fd30decc3","permalink":"https://about.blaok.me/publication/fpgp/","publishdate":"2016-06-21T00:00:00Z","relpermalink":"/publication/fpgp/","section":"publication","summary":"Large-scale graph processing is gaining increasing attentions in many domains. Meanwhile, FPGA provides a power-efficient and highly parallel platform for many applications, and has been applied to custom computing in many domains. In this paper, we describe FPGP (**FP**GA **G**raph **P**rocessing), a streamlined vertex-centric graph processing framework on FPGA, based on the interval-shard structure. FPGP is adaptable to different graph algorithms and users do not need to change the whole implementation on the FPGA. In our implementation, an on-chip parallel graph processor is proposed to both maximize the off-chip bandwidth of graph data and fully utilize the parallelism of graph processing. Meanwhile, we analyze the performance of FPGP and show the scalability of FPGP when the bandwidth of data path increases. FPGP is more power-efficient than single machine systems and scalable to larger graphs compared with other FPGA-based graph systems.","tags":null,"title":"FPGP: Graph Processing Framework on FPGA A Case Study of Breadth-First Search","type":"publication"},{"authors":["**Yuze Chi**","Guohao Dai","Yu Wang","Guangyu Sun","Guoliang Li","Huazhong Yang"],"categories":null,"content":"","date":1463356800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541806895,"objectID":"db6aa360781db8ad944ec57cf417fb30","permalink":"https://about.blaok.me/publication/nxgraph/","publishdate":"2016-05-16T00:00:00Z","relpermalink":"/publication/nxgraph/","section":"publication","summary":"Recent studies show that graph processing systems on a single machine can achieve competitive performance compared with cluster-based graph processing systems. In this paper, we present NXgraph, an efficient graph processing system on a single machine. We propose the Destination-Sorted SubShard (DSSS) structure to store a graph. To ensure graph data access locality and enable fine-grained scheduling, NXgraph divides vertices and edges into intervals and sub-shards. To reduce write conflicts among different threads and achieve a high degree of parallelism, NXgraph sorts edges within each sub-shard according to their destination vertices. Then, three updating strategies, i.e., Single-Phase Update (SPU), Double-Phase Update (DPU), and Mixed-Phase Update (MPU), are proposed in this paper. NXgraph can adaptively choose the fastest strategy for different graph problems according to the graph size and the available memory resources to fully utilize the memory space and reduce the amount of data transfer. All these three strategies exploit streamlined disk access patterns. Extensive experiments on three real-world graphs and five synthetic graphs show that NXgraph outperforms GraphChi, TurboGraph, VENUS, and GridGraph in various situations. Moreover, NXgraph, running on a single commodity PC, can finish an iteration of PageRank on the Twitter graph with 1.5 billion edges in 2.05 seconds; while PowerGraph, a distributed graph processing system, needs 3.6s to finish the same task on a 64-node cluster.","tags":null,"title":"NXgraph: An Efficient Graph Processing System on a Single Machine","type":"publication"},{"authors":["Hai‐Xiao Du","Xu‐Hong Liao","Qi‐Xiang Lin","Gu‐Shu Li","**Yu‐Ze Chi**","Xiang Liu","Hua‐Zhong Yang","Yu Wang","Ming‐Rui Xia"],"categories":null,"content":"","date":1442793600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541806895,"objectID":"3f247185dea57d8b7784ca1b6c6a0d58","permalink":"https://about.blaok.me/publication/mri/","publishdate":"2015-09-21T00:00:00Z","relpermalink":"/publication/mri/","section":"publication","summary":"**Background**: The combination of resting-state functional MRI (R-fMRI) technique and graph theoretical approaches has emerged as a promising tool for characterizing the topological organization of brain networks, that is, functional connectomics. In particular, the construction and analysis of high-resolution brain connectomics at a voxel scale are important because they do not require prior regional parcellations and provide finer spatial information about brain connectivity. However, the test–retest reliability of voxel-based functional connectomics remains largely unclear. **Aims**: This study tended to investigate both short-term (~ 20 min apart) and long-term (6 weeks apart) test–retest (TRT) reliability of graph metrics of voxel-based brain networks. **Methods**: Based on graph theoretical approaches, we analyzed R-fMRI data from 53 young healthy adults who completed two scanning sessions (session 1 included two scans 20 min apart; session 2 included one scan that was performed after an interval of ~ 6 weeks). **Results**: The high-resolution networks exhibited prominent small-world and modular properties and included functional hubs mainly located at the default-mode, salience, and executive control systems. Further analysis revealed that test–retest reliabilities of network metrics were sensitive to the scanning orders and intervals, with fair to excellent long-term reliability between Scan 1 and Scan 3 and lower reliability involving Scan 2. In the long-term case (Scan 1 and Scan 3), most network metrics were generally test–retest reliable, with the highest reliability in global metrics in the clustering coefficient and in the nodal metrics in nodal degree and efficiency. **Conclusion**: We showed high test–retest reliability for graph properties in the high-resolution functional connectomics, which provides important guidance for choosing reliable network metrics and analysis strategies in future studies.","tags":null,"title":"Test–Retest Reliability of Graph Metrics in High-resolution Functional Connectomics: A Resting-State Functional MRI Study","type":"publication"}]